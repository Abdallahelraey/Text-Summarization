{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\AI\\\\NLP\\\\HandsOn\\\\Text Summarization'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    checkpoint: str\n",
    "    max_length: int\n",
    "    min_length: int\n",
    "    output_dir: str\n",
    "    learning_rate: float\n",
    "    train_batch_size: int\n",
    "    eval_batch_size: int\n",
    "    weight_decay: float\n",
    "    save_total_limit: int\n",
    "    num_train_epochs: int\n",
    "    prefix: str\n",
    "    push_to_hub: bool\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TextSummarizer.constants import *\n",
    "from TextSummarizer.utils.file_utils import *\n",
    "from TextSummarizer.utils.config_utils import *\n",
    "from TextSummarizer.utils.model_utils import *\n",
    "from TextSummarizer.utils.lib_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_model_summarizer_config(self) -> TrainingConfig:\n",
    "        config = self.params.model\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_summarize_config = TrainingConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "            checkpoint = config.checkpoint,\n",
    "            max_length = config.max_length,\n",
    "            min_length = config.min_length,\n",
    "            output_dir = config.output_dir,\n",
    "            learning_rate = float(config.learning_rate),\n",
    "            train_batch_size = config.train_batch_size,\n",
    "            eval_batch_size = config.eval_batch_size,\n",
    "            weight_decay = config.weight_decay,\n",
    "            save_total_limit = config.save_total_limit,\n",
    "            num_train_epochs = config.num_train_epochs,\n",
    "            push_to_hub = config.push_to_hub,\n",
    "            device = config.device,\n",
    "            prefix = config.prefix\n",
    "        )\n",
    "\n",
    "        return model_summarize_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AI\\NLP\\HandsOn\\Text Summarization\\textsummarizer-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-25 09:09:14,888: INFO: config: PyTorch version 2.5.1+cu121 available.]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer ,AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import pipeline\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "import nltk\n",
    "from datasets import DatasetDict, load_dataset, load_from_disk \n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from huggingface_hub import HfApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizationModel:\n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        logger.info(\"Initializing SummarizationModel with provided configuration.\")\n",
    "        self.config = config\n",
    "        self.settings = get_settings()\n",
    "        self.model_name = \"TED_FineTuned_google-t5\"\n",
    "        self.data_path = self.config.data_path\n",
    "        self.output_dir = self.config.output_dir\n",
    "        self.device = self.config.device\n",
    "        self.checkpoint = self.config.checkpoint\n",
    "        self.max_length = self.config.max_length\n",
    "        self.min_length = self.config.min_length\n",
    "        self.prefix = self.config.prefix\n",
    "        logger.info(\"Loading tokenizer and model from checkpoint: %s\", self.checkpoint)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.checkpoint)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(self.checkpoint).to(self.device)\n",
    "        logger.info(\"Tokenizer and model loaded successfully.\")\n",
    "        self.data_collator = DataCollatorForSeq2Seq(\n",
    "            tokenizer=self.tokenizer,\n",
    "            model=self.model,\n",
    "            padding=True,\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "        logger.info(\"Data collator initialized.\")\n",
    "        self.training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=self.output_dir,\n",
    "            eval_strategy=\"epoch\",\n",
    "            learning_rate=self.config.learning_rate,\n",
    "            per_device_train_batch_size=self.config.train_batch_size,\n",
    "            per_device_eval_batch_size=self.config.eval_batch_size,\n",
    "            weight_decay=self.config.weight_decay,\n",
    "            save_total_limit=self.config.save_total_limit,\n",
    "            num_train_epochs=self.config.num_train_epochs,\n",
    "            predict_with_generate=True,\n",
    "            fp16=True,\n",
    "            push_to_hub=self.config.push_to_hub,\n",
    "        )\n",
    "        logger.info(\"Training arguments configured.\")\n",
    "\n",
    "    def load_data_into_DatasetDict(self) -> DatasetDict:\n",
    "        logger.info(\"Loading dataset from path: %s\", self.data_path)\n",
    "        loaded_dataset = load_from_disk(self.data_path)\n",
    "        logger.info(\"Dataset loaded successfully with %d splits.\", len(loaded_dataset))\n",
    "        return loaded_dataset\n",
    "\n",
    "    def compute_metrics(self, eval_pred):\n",
    "        logger.info(\"Computing evaluation metrics.\")\n",
    "        rouge = evaluate.load(\"rouge\")\n",
    "        predictions, labels = eval_pred\n",
    "        decoded_preds = self.tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        labels = np.where(labels != -100, labels, self.tokenizer.pad_token_id)\n",
    "        decoded_labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "        prediction_lens = [np.count_nonzero(pred != self.tokenizer.pad_token_id) for pred in predictions]\n",
    "        result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "        logger.info(\"Metrics computed: %s\", result)\n",
    "        return {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "    def model_trainer(self, dataset):\n",
    "        logger.info(\"Initializing Seq2SeqTrainer for training.\")\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=self.model,\n",
    "            args=self.training_args,\n",
    "            train_dataset=dataset[\"train\"],\n",
    "            eval_dataset=dataset[\"test\"],\n",
    "            tokenizer=self.tokenizer,\n",
    "            data_collator=self.data_collator,\n",
    "            compute_metrics=self.compute_metrics,\n",
    "        )\n",
    "        logger.info(\"Training started.\")\n",
    "        trainer.train()\n",
    "        logger.info(\"Training completed.\")\n",
    "        return trainer\n",
    "    \n",
    "\n",
    "    def save_model(self, model):\n",
    "        logger.info(\"Saving model to output directory: %s\", self.output_dir)\n",
    "        model.save_pretrained(self.output_dir)\n",
    "        # model.save_model(self.output_dir)\n",
    "        logger.info(\"Model saved successfully.\")\n",
    "\n",
    "    def upload_trained_model(self, trainer: Trainer):\n",
    "        try:\n",
    "            uploader = HuggingFaceModelUploader(\n",
    "                model_name=self.model_name,\n",
    "                output_dir=self.output_dir,\n",
    "                private=False\n",
    "            )\n",
    "            uploader.run(trainer=trainer)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Upload failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "    def predict(self, text: str) -> str:\n",
    "        logger.info(\"Generating summary for input text.\")\n",
    "        inputs = self.tokenizer(\n",
    "            self.prefix + text,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.device)\n",
    "        summary_ids = self.model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_length=self.max_length,\n",
    "            min_length=self.min_length,\n",
    "            num_beams=4,\n",
    "            length_penalty=2.0,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        logger.info(\"Summary generated: %s\", summary)\n",
    "        return summary\n",
    "\n",
    "    def evaluate_model(self, dataset):\n",
    "        logger.info(\"Evaluating the model on the test dataset.\")\n",
    "        \n",
    "        def tokenize_function(batch):\n",
    "            inputs = self.tokenizer(\n",
    "                [self.prefix + text for text in batch[\"text\"]],\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=None  # Don't convert to tensors in map\n",
    "            )\n",
    "            return inputs\n",
    "\n",
    "        # Tokenize the test dataset\n",
    "        tokenized_dataset = dataset[\"test\"].map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=dataset[\"test\"].column_names\n",
    "        )\n",
    "\n",
    "        # Create dataloader for batched processing\n",
    "        eval_dataloader = DataLoader(\n",
    "            tokenized_dataset,\n",
    "            batch_size=self.training_args.per_device_eval_batch_size,\n",
    "            collate_fn=self.data_collator\n",
    "        )\n",
    "\n",
    "        predictions = []\n",
    "        references = dataset[\"test\"][\"summary\"]\n",
    "\n",
    "        # Generate predictions in batches\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in eval_dataloader:\n",
    "                # Only move non-None tensor values to device\n",
    "                processed_batch = {}\n",
    "                for k, v in batch.items():\n",
    "                    if v is not None and isinstance(v, torch.Tensor):\n",
    "                        processed_batch[k] = v.to(self.device)\n",
    "                    else:\n",
    "                        processed_batch[k] = v\n",
    "                \n",
    "                # Ensure we have input_ids\n",
    "                if \"input_ids\" not in processed_batch or processed_batch[\"input_ids\"] is None:\n",
    "                    logger.warning(\"Skipping batch due to missing input_ids\")\n",
    "                    continue\n",
    "                    \n",
    "                outputs = self.model.generate(\n",
    "                    input_ids=processed_batch[\"input_ids\"],\n",
    "                    max_length=self.max_length,\n",
    "                    num_beams=4,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "                predictions.extend(\n",
    "                    self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "                )\n",
    "\n",
    "        # Use the existing compute_metrics logic\n",
    "        rouge = evaluate.load(\"rouge\")\n",
    "        scores = rouge.compute(\n",
    "            predictions=predictions,\n",
    "            references=references,\n",
    "            use_stemmer=True\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Evaluation completed with scores: %s\", scores)\n",
    "        return {k: round(v, 4) for k, v in scores.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load configuration and initialize summarization model\n",
    "    config = ConfigurationManager()\n",
    "    model_summarizer_config = config.get_model_summarizer_config()\n",
    "    model_summarizer = SummarizationModel(config=model_summarizer_config)\n",
    "    \n",
    "    # Load dataset\n",
    "    data_set = model_summarizer.load_data_into_DatasetDict()\n",
    "    \n",
    "    # Train the summarizer\n",
    "    trained_summarizer = model_summarizer.model_trainer(data_set)\n",
    "    \n",
    "    # Pushing the model to the Huggig Face Hub\n",
    "    model_summarizer.push_model_to_hub(trained_summarizer)\n",
    "    \n",
    "    # Save the trained model\n",
    "    model_summarizer.save_model(trained_summarizer)\n",
    "    \n",
    "    # Upload model to hub\n",
    "    model_summarizer.upload_trained_model(trained_summarizer)\n",
    "    \n",
    "    # Evaluate the model on the test dataset\n",
    "    logger.info(\"Starting model evaluation...\")\n",
    "    evaluation_results = model_summarizer.evaluate_model(data_set)\n",
    "    logger.info(f\"Evaluation Results: {evaluation_results}\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"An error occurred: {e}\")\n",
    "    raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prediction on a single example\n",
    "logger.info(\"Starting prediction...\")\n",
    "sample_text = \"Imagine a world overwhelmed by information, where sifting through endless articles, reports, and data consumes valuable time and energy. AI summarization offers a powerful solution, employing natural language processing to condense lengthy texts into concise summaries. These systems utilize two primary approaches: extractive summarization, selecting key sentences directly from the source, and abstractive summarization, generating new, more human-like summaries that capture the core meaning. This technology finds diverse applications, from streamlining news consumption and accelerating research analysis to optimizing business workflows by summarizing meetings and customer feedback. While challenges remain in handling complex language, nuanced meanings, and ensuring factual accuracy, ongoing research continually improves the fluency, coherence, and contextual understanding of AI-generated summaries. Ultimately, AI summarization promises to revolutionize how we process information, empowering us to access key insights quickly and efficiently, unlocking the potential of knowledge for a more informed future.\"\n",
    "predicted_summary = model_summarizer.predict(sample_text)\n",
    "logger.info(f\"Predicted Summary: {predicted_summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM , AutoTokenizer\n",
    "\n",
    "# Load your model and tokenizer\n",
    "model_name = \"AbdallahElraey/HFmodels\"  # Your model path from the image\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Example text for inference\n",
    "text = \"Imagine a world overwhelmed by information, where sifting through endless articles, reports, and data consumes valuable time and energy. AI summarization offers a powerful solution, employing natural language processing to condense lengthy texts into concise summaries. These systems utilize two primary approaches: extractive summarization, selecting key sentences directly from the source, and abstractive summarization, generating new, more human-like summaries that capture the core meaning. This technology finds diverse applications, from streamlining news consumption and accelerating research analysis to optimizing business workflows by summarizing meetings and customer feedback. While challenges remain in handling complex language, nuanced meanings, and ensuring factual accuracy, ongoing research continually improves the fluency, coherence, and contextual understanding of AI-generated summaries. Ultimately, AI summarization promises to revolutionize how we process information, empowering us to access key insights quickly and efficiently, unlocking the potential of knowledge for a more informed future.\"\n",
    "\n",
    "# Tokenize and generate\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "outputs = model.generate(**inputs)\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textsummarizer-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
