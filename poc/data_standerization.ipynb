{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\AI\\\\NLP\\\\HandsOn\\\\Text Summarization'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class DataStandardizationConfig:\n",
    "    input_file_directory: Path\n",
    "    output_dir: Path\n",
    "    output_file: Path\n",
    "    ALL_REQUIRED_FILES: list\n",
    "    text_columns: list\n",
    "    relevant_fields: list\n",
    "    merging_key: str\n",
    "    nltk_dir: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TextSummarizer.constants import *\n",
    "from TextSummarizer.utils.file_utils import *\n",
    "from TextSummarizer.utils.config_utils import *\n",
    "from TextSummarizer.utils.lib_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "    def get_data_standardization_config(self) -> DataStandardizationConfig:\n",
    "        config = self.config.data_standardization\n",
    "        create_directories([config.output_dir])\n",
    "        data_standardization_config = DataStandardizationConfig(\n",
    "            input_file_directory=config.input_file_directory,\n",
    "            output_dir = config.output_dir,\n",
    "            output_file = config.output_file,\n",
    "            ALL_REQUIRED_FILES=config.ALL_REQUIRED_FILES,\n",
    "            text_columns  = config.text_columns,\n",
    "            relevant_fields = config.relevant_fields,\n",
    "            merging_key = config.merging_key,\n",
    "            nltk_dir = config.nltk_dir\n",
    "            #output_file_path=os.path.join(config.output_dir, config.output_file)\n",
    "   \n",
    "        )\n",
    "        return data_standardization_config\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from typing import Set, Optional, List, Union\n",
    "from  TextSummarizer.logging import logger\n",
    "\n",
    "\n",
    "class DataStandardization:\n",
    "    def __init__(self, \n",
    "                 config: DataStandardizationConfig,\n",
    "                 remove_stops: bool = True,\n",
    "                 lemmatize: bool = True,\n",
    "                 custom_stopwords: Optional[Set[str]] = None):\n",
    "        logger.info(\"Initializing DataStandardization with config\")\n",
    "        self.config = config\n",
    "        self.remove_stops = remove_stops\n",
    "        self.lemmatize = lemmatize\n",
    "        self.custom_stopwords = custom_stopwords\n",
    "        self.main_csv_path = os.path.join(self.config.input_file_directory,self.config.ALL_REQUIRED_FILES[0]) \n",
    "        self.transcripts_csv_path = os.path.join(self.config.input_file_directory,self.config.ALL_REQUIRED_FILES[1])\n",
    "        self.text_columns = self.config.text_columns\n",
    "        logger.debug(f\"Set up paths - Main CSV: {self.main_csv_path}, Transcripts CSV: {self.transcripts_csv_path}\")\n",
    "        self.setup_nlp_utilities()\n",
    "        \n",
    "    def setup_nlp_utilities(self):\n",
    "        logger.info(\"Setting up NLP utilities\")\n",
    "        setup_nltk_environment(self.config.nltk_dir)\n",
    "        download_nltk_models(self.config.nltk_dir)\n",
    "        logger.info(\"NLP utilities setup completed\")\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        logger.debug(\"Starting text cleaning process\")\n",
    "        if pd.isna(text):\n",
    "            logger.warning(\"Received NA value for text cleaning\")\n",
    "            return \"\"\n",
    "            \n",
    "        # Convert to lowercase and string type\n",
    "        text = str(text).lower()\n",
    "        \n",
    "        # Remove special characters and digits\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        logger.debug(\"Text cleaning completed\")\n",
    "        return text\n",
    "\n",
    "    def remove_stopwords(self, text: str, custom_stopwords: Optional[Set[str]] = None) -> str:\n",
    "        logger.debug(\"Starting stopwords removal\")\n",
    "        if not text:\n",
    "            logger.warning(\"Received empty text for stopwords removal\")\n",
    "            return \"\"\n",
    "            \n",
    "        try:\n",
    "            from nltk.corpus import stopwords\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            if custom_stopwords:\n",
    "                stop_words.update(custom_stopwords)\n",
    "                logger.debug(f\"Added {len(custom_stopwords)} custom stopwords\")\n",
    "        except LookupError:\n",
    "            logger.info(\"Downloading stopwords...\")\n",
    "            nltk.download('stopwords')\n",
    "            from nltk.corpus import stopwords\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        words = text.split()\n",
    "        filtered_words = [word for word in words if word not in stop_words]\n",
    "        logger.debug(f\"Removed {len(words) - len(filtered_words)} stopwords\")\n",
    "        return ' '.join(filtered_words)\n",
    "\n",
    "    def lemmatize_text(self, text: str) -> str:\n",
    "        logger.debug(\"Starting text lemmatization\")\n",
    "        if not text:\n",
    "            logger.warning(\"Received empty text for lemmatization\")\n",
    "            return \"\"\n",
    "            \n",
    "        try:\n",
    "            from nltk.stem import WordNetLemmatizer\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "        except LookupError:\n",
    "            logger.info(\"Downloading required models for lemmatization...\")\n",
    "            nltk.download('wordnet')\n",
    "            from nltk.stem import WordNetLemmatizer\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        words = text.split()\n",
    "        lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "        logger.debug(\"Lemmatization completed\")\n",
    "        return ' '.join(lemmatized_words)\n",
    "\n",
    "    def standardize_text(self, text: str) -> str:\n",
    "        logger.debug(\"Starting text standardization\")\n",
    "        # Clean the text first\n",
    "        text = self.clean_text(text)\n",
    "        \n",
    "        # Remove stopwords if requested\n",
    "        if self.remove_stops:\n",
    "            text = self.remove_stopwords(text, self.custom_stopwords)\n",
    "        \n",
    "        # Lemmatize if requested\n",
    "        if self.lemmatize:\n",
    "            text = self.lemmatize_text(text)\n",
    "        \n",
    "        logger.debug(\"Text standardization completed\")\n",
    "        return text\n",
    "\n",
    "    def process_dataframe(self, \n",
    "                         df: pd.DataFrame, \n",
    "                         suffix: str = '_standardized') -> pd.DataFrame:\n",
    "        logger.info(f\"Processing DataFrame with {len(df)} rows\")\n",
    "        # Create a copy of the DataFrame to avoid modifying the original\n",
    "        result_df = df.copy()\n",
    "        \n",
    "        # Process each text column\n",
    "        for column in self.text_columns:\n",
    "            if column not in df.columns:\n",
    "                logger.error(f\"Column '{column}' not found in DataFrame\")\n",
    "                raise ValueError(f\"Column '{column}' not found in DataFrame\")\n",
    "                \n",
    "            # Create new column name\n",
    "            new_column = f\"{column}{suffix}\"\n",
    "            logger.info(f\"Processing column: {column} -> {new_column}\")\n",
    "            \n",
    "            # Apply standardization to the column\n",
    "            result_df[new_column] = df[column].apply(self.standardize_text)\n",
    "            logger.debug(f\"Completed processing column: {column}\")\n",
    "        \n",
    "        logger.info(\"DataFrame processing completed\")\n",
    "        return result_df\n",
    "\n",
    "    def load_and_prepare_data(self) -> pd.DataFrame:\n",
    "        logger.info(\"Starting data loading and preparation\")\n",
    "        # Load main data\n",
    "        logger.debug(f\"Loading main data from {self.main_csv_path}\")\n",
    "        df_main = pd.read_csv(self.main_csv_path)\n",
    "        logger.info(f\"Loaded main data with {len(df_main)} rows\")\n",
    "        \n",
    "        # Select relevant fields\n",
    "        relevant_fields = self.config.relevant_fields\n",
    "        df_main = df_main[relevant_fields]\n",
    "        logger.debug(f\"Selected {len(relevant_fields)} relevant fields\")\n",
    "        \n",
    "        # Load transcripts\n",
    "        logger.debug(f\"Loading transcripts from {self.transcripts_csv_path}\")\n",
    "        df_transcripts = pd.read_csv(self.transcripts_csv_path)\n",
    "        logger.info(f\"Loaded transcripts with {len(df_transcripts)} rows\")\n",
    "        \n",
    "        # Merge dataframes\n",
    "        merging_key = self.config.merging_key\n",
    "        logger.debug(f\"Merging dataframes on key: {merging_key}\")\n",
    "        merged_df = df_main.merge(df_transcripts, on=merging_key).drop(merging_key, axis=1)\n",
    "        logger.info(f\"Merged DataFrame has {len(merged_df)} rows\")\n",
    "        \n",
    "        return merged_df\n",
    "\n",
    "    def save_data(self, data):\n",
    "        logger.info(f\"Saving standardized data to {self.config.output_file}\")\n",
    "        try:\n",
    "            # Save the standardized data\n",
    "            data.to_csv(self.config.output_file, index=False)\n",
    "            logger.info(\"Data successfully saved\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving data: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-08 02:28:09,447: INFO: config_utils: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-12-08 02:28:09,452: INFO: config_utils: yaml file: params.yaml loaded successfully]\n",
      "[2024-12-08 02:28:09,453: INFO: file_utils: created directory at: artifacts]\n",
      "[2024-12-08 02:28:09,454: INFO: file_utils: created directory at: artifacts/data_standardization]\n",
      "[2024-12-08 02:28:09,455: INFO: 1274383872: Initializing DataStandardization with config]\n",
      "[2024-12-08 02:28:09,456: INFO: 1274383872: Setting up NLP utilities]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-08 02:28:11,746: INFO: lib_utils: Successfully downloaded 'punkt' to artifacts/models]\n",
      "[2024-12-08 02:28:11,816: INFO: lib_utils: Successfully downloaded 'stopwords' to artifacts/models]\n",
      "[2024-12-08 02:28:11,979: INFO: lib_utils: Successfully downloaded 'averaged_perceptron_tagger' to artifacts/models]\n",
      "[2024-12-08 02:28:12,408: INFO: lib_utils: Successfully downloaded 'wordnet' to artifacts/models]\n",
      "[2024-12-08 02:28:12,545: INFO: lib_utils: Successfully downloaded 'words' to artifacts/models]\n",
      "[2024-12-08 02:28:12,545: INFO: 1274383872: NLP utilities setup completed]\n",
      "[2024-12-08 02:28:12,546: INFO: 1274383872: Starting data loading and preparation]\n",
      "[2024-12-08 02:28:12,915: INFO: 1274383872: Loaded main data with 2550 rows]\n",
      "[2024-12-08 02:28:13,250: INFO: 1274383872: Loaded transcripts with 2467 rows]\n",
      "[2024-12-08 02:28:13,254: INFO: 1274383872: Merged DataFrame has 2467 rows]\n",
      "[2024-12-08 02:28:13,255: INFO: 1274383872: Processing DataFrame with 2467 rows]\n",
      "[2024-12-08 02:28:13,256: INFO: 1274383872: Processing column: description -> description_standardized]\n",
      "[2024-12-08 02:28:16,880: INFO: 1274383872: Processing column: title -> title_standardized]\n",
      "[2024-12-08 02:28:16,882: WARNING: 1274383872: Received empty text for lemmatization]\n",
      "[2024-12-08 02:28:17,234: WARNING: 1274383872: Received empty text for lemmatization]\n",
      "[2024-12-08 02:28:17,426: INFO: 1274383872: Processing column: transcript -> transcript_standardized]\n",
      "[2024-12-08 02:28:28,524: INFO: 1274383872: DataFrame processing completed]\n",
      "[2024-12-08 02:28:28,525: INFO: 1274383872: Saving standardized data to standardized_data.csv]\n",
      "[2024-12-08 02:28:29,735: INFO: 1274383872: Data successfully saved]\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    standardization_config = ConfigurationManager().get_data_standardization_config()\n",
    "    standardizer = DataStandardization(config=standardization_config, remove_stops=True,  lemmatize=True)\n",
    "    df = standardizer.load_and_prepare_data()\n",
    "    processed_df = standardizer.process_dataframe(df)\n",
    "    standardizer.save_data(processed_df)\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textsummarizer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
