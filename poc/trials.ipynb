{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Welcome to Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AI\\NLP\\HandsOn\\Text Summarization\\textsummarizer-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "d:\\AI\\NLP\\HandsOn\\Text Summarization\\textsummarizer-env\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\enay9\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9801173806190491}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "clssifier = pipeline(\"sentiment-analysis\")\n",
    "res = clssifier(\"I want you to  summarize educatinooal content for me\")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AI\\NLP\\HandsOn\\Text Summarization\\textsummarizer-env\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\enay9\\.cache\\huggingface\\hub\\models--distilgpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'In a worled where education is inivitable as an enterpunoot you have to keep in place the great knowledge, knowledge that is not the quality, the perfect education, education that is good. It is a spiritual or moral need for teachers and teachers and teachers, a spiritual or moral need for teachers and teachers of all classes, at all levels of learning and instruction for everyone: to be able to live, live and work in full power, without any means of oppression. Without such knowledge'}, {'generated_text': 'In a worled where education is inivitable as an enterpunoot you have to start using something to achieve goals, goals, and achievements you can achieve from the basics to the technical knowledge you can achieve through the process. You can work with those qualities to become your success. This can be difficult, if not impossible, for both young and adults. Your family often uses the concept of having a child who does not have one but a single father. This would be true for parents at the'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator  = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "\n",
    "res= generator(\n",
    "    \"In a worled where education is inivitable as an enterpunoot you have to\",\n",
    "    max_length=100,\n",
    "    num_return_sequences=2,\n",
    ") \n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "d:\\AI\\NLP\\HandsOn\\Text Summarization\\textsummarizer-env\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\enay9\\.cache\\huggingface\\hub\\models--facebook--bart-large-mnli. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': 'I want to classify the following article into educational content where yu summarise books', 'labels': ['education', 'business', 'politics'], 'scores': [0.9950029850006104, 0.003552044974640012, 0.001444946276023984]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "\n",
    "res = classifier(\n",
    "    \"I want to classify the following article into educational content where yu summarise books\",\n",
    "    [\"education\", \"politics\", \"business\"],\n",
    ")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers A generic approach with more degrees of freedom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AI\\NLP\\HandsOn\\Text Summarization\\textsummarizer-env\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\enay9\\.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9801173806190491}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "model_name =  \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "res = classifier(\"I want you to  summarize educatinooal content for me\")\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A glimps of the Transformer tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1045, 2215, 2017, 2000, 7680, 7849, 4697, 2023, 3720, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['i', 'want', 'you', 'to', 'sum', '##mar', '##ize', 'this', 'article']\n",
      "30522\n",
      "[1045, 2215, 2017, 2000, 7680, 7849, 4697, 2023, 3720]\n",
      "i want you to summarize this article\n"
     ]
    }
   ],
   "source": [
    "sequence = \"I want you to summarize this article\"\n",
    "res = tokenizer(sequence)\n",
    "print(res)\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "print(tokens)\n",
    "vocab = tokenizer.get_vocab()\n",
    "print(len(vocab))\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)\n",
    "decoded_strings = tokenizer.decode(ids)\n",
    "print(decoded_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A practical guidee for the complete proocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.964401125907898}, {'label': 'POSITIVE', 'score': 0.9213657975196838}]\n",
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "tensor([0, 1])\n",
      "tensor([[0.9644, 0.0356],\n",
      "        [0.0786, 0.9214]])\n",
      "[[0.964401125907898, 0.03559890761971474], [0.07863417267799377, 0.9213657975196838]]\n",
      "Sentence: I have been doing this for about 3 yyears now and it is time to take a  free time to reflect upon what i have done on those uears s so far - Label: 0 - Score: [0.964401125907898, 0.03559890761971474]\n",
      "Sentence: Now  in htis age i think the best next step  is to reflect to geada head  inthe near  future as  it  will work as inflection point  that  can give you all the momentumthat  you need - Label: 1 - Score: [0.07863417267799377, 0.9213657975196838]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "model_name =  \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "X_train =  [\"I have been doing this for about 3 yyears now and it is time to take a  free time to reflect upon what i have done on those uears s so far\",\n",
    "            \"Now  in htis age i think the best next step  is to reflect to geada head  inthe near  future as  it  will work as inflection point  that  can give you all the momentumthat  you need\"]\n",
    "\n",
    "res = classifier(X_train)\n",
    "print(res)\n",
    "\n",
    "\n",
    "batch = tokenizer(X_train, padding=True,truncation=True,max_length=512, return_tensors=\"pt\")\n",
    "print(batch.keys())\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**batch)\n",
    "    logits = outputs.logits\n",
    "    predicted_labels = torch.argmax(logits, dim=1)\n",
    "    print(predicted_labels)\n",
    "    predicted_scores = torch.softmax(logits, dim=1)\n",
    "    print(predicted_scores)\n",
    "    predicted_scores = predicted_scores.tolist()\n",
    "    print(predicted_scores)\n",
    "    for i, (sentence, label, score) in enumerate(zip(X_train, predicted_labels, predicted_scores)):\n",
    "        print(f\"Sentence: {sentence} - Label: {label} - Score: {score}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"Saved\"\n",
    "tokenizer.save_pretrained(data_directory)\n",
    "model.save_pretrained(data_directory)\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(data_directory)\n",
    "mod = AutoModelForSequenceClassification(data_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Prepreparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\AI\\\\NLP\\\\HandsOn\\\\Text Summarization'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "data_directory = \"artifacts\\data_standardization\\standardized_data.csv\"\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=data_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['description', 'tags', 'title', 'ratings', 'transcript', 'description_standardized', 'title_standardized', 'transcript_standardized'],\n",
       "        num_rows: 2467\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': 'Sir Ken Robinson makes an entertaining and profoundly moving case for creating an education system that nurtures (rather than undermines) creativity.',\n",
       " 'tags': \"['children', 'creativity', 'culture', 'dance', 'education', 'parenting', 'teaching']\",\n",
       " 'title': 'Do schools kill creativity?',\n",
       " 'ratings': \"[{'id': 7, 'name': 'Funny', 'count': 19645}, {'id': 1, 'name': 'Beautiful', 'count': 4573}, {'id': 9, 'name': 'Ingenious', 'count': 6073}, {'id': 3, 'name': 'Courageous', 'count': 3253}, {'id': 11, 'name': 'Longwinded', 'count': 387}, {'id': 2, 'name': 'Confusing', 'count': 242}, {'id': 8, 'name': 'Informative', 'count': 7346}, {'id': 22, 'name': 'Fascinating', 'count': 10581}, {'id': 21, 'name': 'Unconvincing', 'count': 300}, {'id': 24, 'name': 'Persuasive', 'count': 10704}, {'id': 23, 'name': 'Jaw-dropping', 'count': 4439}, {'id': 25, 'name': 'OK', 'count': 1174}, {'id': 26, 'name': 'Obnoxious', 'count': 209}, {'id': 10, 'name': 'Inspiring', 'count': 24924}]\",\n",
       " 'transcript': 'Good morning. How are you?(Laughter)It\\'s been great, hasn\\'t it? I\\'ve been blown away by the whole thing. In fact, I\\'m leaving.(Laughter)There have been three themes running through the conference which are relevant to what I want to talk about. One is the extraordinary evidence of human creativity in all of the presentations that we\\'ve had and in all of the people here. Just the variety of it and the range of it. The second is that it\\'s put us in a place where we have no idea what\\'s going to happen, in terms of the future. No idea how this may play out.I have an interest in education. Actually, what I find is everybody has an interest in education. Don\\'t you? I find this very interesting. If you\\'re at a dinner party, and you say you work in education — Actually, you\\'re not often at dinner parties, frankly.(Laughter)If you work in education, you\\'re not asked.(Laughter)And you\\'re never asked back, curiously. That\\'s strange to me. But if you are, and you say to somebody, you know, they say, \"What do you do?\" and you say you work in education, you can see the blood run from their face. They\\'re like, \"Oh my God,\" you know, \"Why me?\"(Laughter)\"My one night out all week.\"(Laughter)But if you ask about their education, they pin you to the wall. Because it\\'s one of those things that goes deep with people, am I right? Like religion, and money and other things. So I have a big interest in education, and I think we all do. We have a huge vested interest in it, partly because it\\'s education that\\'s meant to take us into this future that we can\\'t grasp. If you think of it, children starting school this year will be retiring in 2065. Nobody has a clue, despite all the expertise that\\'s been on parade for the past four days, what the world will look like in five years\\' time. And yet we\\'re meant to be educating them for it. So the unpredictability, I think, is extraordinary.And the third part of this is that we\\'ve all agreed, nonetheless, on the really extraordinary capacities that children have — their capacities for innovation. I mean, Sirena last night was a marvel, wasn\\'t she? Just seeing what she could do. And she\\'s exceptional, but I think she\\'s not, so to speak, exceptional in the whole of childhood. What you have there is a person of extraordinary dedication who found a talent. And my contention is, all kids have tremendous talents. And we squander them, pretty ruthlessly.So I want to talk about education and I want to talk about creativity. My contention is that creativity now is as important in education as literacy, and we should treat it with the same status.(Applause) Thank you.(Applause)That was it, by the way. Thank you very much.(Laughter)So, 15 minutes left.(Laughter)Well, I was born... no.(Laughter)I heard a great story recently — I love telling it — of a little girl who was in a drawing lesson. She was six, and she was at the back, drawing, and the teacher said this girl hardly ever paid attention, and in this drawing lesson, she did. The teacher was fascinated. She went over to her, and she said, \"What are you drawing?\" And the girl said, \"I\\'m drawing a picture of God.\" And the teacher said, \"But nobody knows what God looks like.\" And the girl said, \"They will, in a minute.\"(Laughter)When my son was four in England — Actually, he was four everywhere, to be honest.(Laughter)If we\\'re being strict about it, wherever he went, he was four that year. He was in the Nativity play. Do you remember the story?(Laughter)No, it was big, it was a big story. Mel Gibson did the sequel, you may have seen it.(Laughter)\"Nativity II.\" But James got the part of Joseph, which we were thrilled about. We considered this to be one of the lead parts. We had the place crammed full of agents in T-shirts: \"James Robinson IS Joseph!\" (Laughter) He didn\\'t have to speak, but you know the bit where the three kings come in? They come in bearing gifts, gold, frankincense and myrrh. This really happened. We were sitting there and I think they just went out of sequence, because we talked to the little boy afterward and we said, \"You OK with that?\" And he said, \"Yeah, why? Was that wrong?\" They just switched. The three boys came in, four-year-olds with tea towels on their heads, and they put these boxes down, and the first boy said, \"I bring you gold.\" And the second boy said, \"I bring you myrrh.\" And the third boy said, \"Frank sent this.\"(Laughter)What these things have in common is that kids will take a chance. If they don\\'t know, they\\'ll have a go. Am I right? They\\'re not frightened of being wrong. I don\\'t mean to say that being wrong is the same thing as being creative. What we do know is, if you\\'re not prepared to be wrong, you\\'ll never come up with anything original — if you\\'re not prepared to be wrong. And by the time they get to be adults, most kids have lost that capacity. They have become frightened of being wrong. And we run our companies like this. We stigmatize mistakes. And we\\'re now running national education systems where mistakes are the worst thing you can make. And the result is that we are educating people out of their creative capacities.Picasso once said this, he said that all children are born artists. The problem is to remain an artist as we grow up. I believe this passionately, that we don\\'t grow into creativity, we grow out of it. Or rather, we get educated out of it. So why is this?I lived in Stratford-on-Avon until about five years ago. In fact, we moved from Stratford to Los Angeles. So you can imagine what a seamless transition that was.(Laughter)Actually, we lived in a place called Snitterfield, just outside Stratford, which is where Shakespeare\\'s father was born. Are you struck by a new thought? I was. You don\\'t think of Shakespeare having a father, do you? Do you? Because you don\\'t think of Shakespeare being a child, do you? Shakespeare being seven? I never thought of it. I mean, he was seven at some point. He was in somebody\\'s English class, wasn\\'t he?(Laughter)How annoying would that be?(Laughter)\"Must try harder.\"(Laughter)Being sent to bed by his dad, you know, to Shakespeare, \"Go to bed, now! And put the pencil down.\"(Laughter)\"And stop speaking like that.\"(Laughter)\"It\\'s confusing everybody.\"(Laughter)Anyway, we moved from Stratford to Los Angeles, and I just want to say a word about the transition. My son didn\\'t want to come. I\\'ve got two kids; he\\'s 21 now, my daughter\\'s 16. He didn\\'t want to come to Los Angeles. He loved it, but he had a girlfriend in England. This was the love of his life, Sarah. He\\'d known her for a month.(Laughter)Mind you, they\\'d had their fourth anniversary, because it\\'s a long time when you\\'re 16. He was really upset on the plane, he said, \"I\\'ll never find another girl like Sarah.\" And we were rather pleased about that, frankly —(Laughter)Because she was the main reason we were leaving the country.(Laughter)But something strikes you when you move to America and travel around the world: Every education system on Earth has the same hierarchy of subjects. Every one. Doesn\\'t matter where you go. You\\'d think it would be otherwise, but it isn\\'t. At the top are mathematics and languages, then the humanities, and at the bottom are the arts. Everywhere on Earth. And in pretty much every system too, there\\'s a hierarchy within the arts. Art and music are normally given a higher status in schools than drama and dance. There isn\\'t an education system on the planet that teaches dance everyday to children the way we teach them mathematics. Why? Why not? I think this is rather important. I think math is very important, but so is dance. Children dance all the time if they\\'re allowed to, we all do. We all have bodies, don\\'t we? Did I miss a meeting?(Laughter)Truthfully, what happens is, as children grow up, we start to educate them progressively from the waist up. And then we focus on their heads. And slightly to one side.If you were to visit education, as an alien, and say \"What\\'s it for, public education?\" I think you\\'d have to conclude, if you look at the output, who really succeeds by this, who does everything that they should, who gets all the brownie points, who are the winners — I think you\\'d have to conclude the whole purpose of public education throughout the world is to produce university professors. Isn\\'t it? They\\'re the people who come out the top. And I used to be one, so there.(Laughter)And I like university professors, but you know, we shouldn\\'t hold them up as the high-water mark of all human achievement. They\\'re just a form of life, another form of life. But they\\'re rather curious, and I say this out of affection for them. There\\'s something curious about professors in my experience — not all of them, but typically, they live in their heads. They live up there, and slightly to one side. They\\'re disembodied, you know, in a kind of literal way. They look upon their body as a form of transport for their heads.(Laughter)Don\\'t they? It\\'s a way of getting their head to meetings.(Laughter)If you want real evidence of out-of-body experiences, get yourself along to a residential conference of senior academics, and pop into the discotheque on the final night.(Laughter)And there, you will see it. Grown men and women writhing uncontrollably, off the beat.(Laughter)Waiting until it ends so they can go home and write a paper about it.(Laughter)Our education system is predicated on the idea of academic ability. And there\\'s a reason. Around the world, there were no public systems of education, really, before the 19th century. They all came into being to meet the needs of industrialism. So the hierarchy is rooted on two ideas.Number one, that the most useful subjects for work are at the top. So you were probably steered benignly away from things at school when you were a kid, things you liked, on the grounds that you would never get a job doing that. Is that right? Don\\'t do music, you\\'re not going to be a musician; don\\'t do art, you won\\'t be an artist. Benign advice — now, profoundly mistaken. The whole world is engulfed in a revolution.And the second is academic ability, which has really come to dominate our view of intelligence, because the universities designed the system in their image. If you think of it, the whole system of public education around the world is a protracted process of university entrance. And the consequence is that many highly-talented, brilliant, creative people think they\\'re not, because the thing they were good at at school wasn\\'t valued, or was actually stigmatized. And I think we can\\'t afford to go on that way.In the next 30 years, according to UNESCO, more people worldwide will be graduating through education than since the beginning of history. More people, and it\\'s the combination of all the things we\\'ve talked about — technology and its transformation effect on work, and demography and the huge explosion in population.Suddenly, degrees aren\\'t worth anything. Isn\\'t that true? When I was a student, if you had a degree, you had a job. If you didn\\'t have a job, it\\'s because you didn\\'t want one. And I didn\\'t want one, frankly. (Laughter) But now kids with degrees are often heading home to carry on playing video games, because you need an MA where the previous job required a BA, and now you need a PhD for the other. It\\'s a process of academic inflation. And it indicates the whole structure of education is shifting beneath our feet. We need to radically rethink our view of intelligence.We know three things about intelligence. One, it\\'s diverse. We think about the world in all the ways that we experience it. We think visually, we think in sound, we think kinesthetically. We think in abstract terms, we think in movement. Secondly, intelligence is dynamic. If you look at the interactions of a human brain, as we heard yesterday from a number of presentations, intelligence is wonderfully interactive. The brain isn\\'t divided into compartments. In fact, creativity — which I define as the process of having original ideas that have value — more often than not comes about through the interaction of different disciplinary ways of seeing things.By the way, there\\'s a shaft of nerves that joins the two halves of the brain called the corpus callosum. It\\'s thicker in women. Following off from Helen yesterday, this is probably why women are better at multi-tasking. Because you are, aren\\'t you? There\\'s a raft of research, but I know it from my personal life. If my wife is cooking a meal at home — which is not often, thankfully.(Laughter)No, she\\'s good at some things, but if she\\'s cooking, she\\'s dealing with people on the phone, she\\'s talking to the kids, she\\'s painting the ceiling, she\\'s doing open-heart surgery over here. If I\\'m cooking, the door is shut, the kids are out, the phone\\'s on the hook, if she comes in I get annoyed. I say, \"Terry, please, I\\'m trying to fry an egg in here.\"(Laughter)\"Give me a break.\"(Laughter)Actually, do you know that old philosophical thing, if a tree falls in a forest and nobody hears it, did it happen? Remember that old chestnut? I saw a great t-shirt recently, which said, \"If a man speaks his mind in a forest, and no woman hears him, is he still wrong?\"(Laughter)And the third thing about intelligence is, it\\'s distinct. I\\'m doing a new book at the moment called \"Epiphany,\" which is based on a series of interviews with people about how they discovered their talent. I\\'m fascinated by how people got to be there. It\\'s really prompted by a conversation I had with a wonderful woman who maybe most people have never heard of, Gillian Lynne. Have you heard of her? Some have. She\\'s a choreographer, and everybody knows her work. She did \"Cats\" and \"Phantom of the Opera.\" She\\'s wonderful. I used to be on the board of The Royal Ballet, as you can see. Anyway, Gillian and I had lunch one day and I said, \"How did you get to be a dancer?\" It was interesting. When she was at school, she was really hopeless. And the school, in the \\'30s, wrote to her parents and said, \"We think Gillian has a learning disorder.\" She couldn\\'t concentrate; she was fidgeting. I think now they\\'d say she had ADHD. Wouldn\\'t you? But this was the 1930s, and ADHD hadn\\'t been invented at this point. It wasn\\'t an available condition.(Laughter)People weren\\'t aware they could have that.(Laughter)Anyway, she went to see this specialist. So, this oak-paneled room, and she was there with her mother, and she was led and sat on this chair at the end, and she sat on her hands for 20 minutes while this man talked to her mother about the problems Gillian was having at school. Because she was disturbing people; her homework was always late; and so on, little kid of eight. In the end, the doctor went and sat next to Gillian, and said, \"I\\'ve listened to all these things your mother\\'s told me, I need to speak to her privately. Wait here. We\\'ll be back; we won\\'t be very long,\" and they went and left her.But as they went out of the room, he turned on the radio that was sitting on his desk. And when they got out, he said to her mother, \"Just stand and watch her.\" And the minute they left the room, she was on her feet, moving to the music. And they watched for a few minutes and he turned to her mother and said, \"Mrs. Lynne, Gillian isn\\'t sick; she\\'s a dancer. Take her to a dance school.\"I said, \"What happened?\" She said, \"She did. I can\\'t tell you how wonderful it was. We walked in this room and it was full of people like me. People who couldn\\'t sit still. People who had to move to think.\" Who had to move to think. They did ballet, they did tap, jazz; they did modern; they did contemporary. She was eventually auditioned for the Royal Ballet School; she became a soloist; she had a wonderful career at the Royal Ballet. She eventually graduated from the Royal Ballet School, founded the Gillian Lynne Dance Company, met Andrew Lloyd Webber. She\\'s been responsible for some of the most successful musical theater productions in history, she\\'s given pleasure to millions, and she\\'s a multi-millionaire. Somebody else might have put her on medication and told her to calm down.(Applause)What I think it comes to is this: Al Gore spoke the other night about ecology and the revolution that was triggered by Rachel Carson. I believe our only hope for the future is to adopt a new conception of human ecology, one in which we start to reconstitute our conception of the richness of human capacity. Our education system has mined our minds in the way that we strip-mine the earth: for a particular commodity. And for the future, it won\\'t serve us. We have to rethink the fundamental principles on which we\\'re educating our children.There was a wonderful quote by Jonas Salk, who said, \"If all the insects were to disappear from the Earth, within 50 years all life on Earth would end. If all human beings disappeared from the Earth, within 50 years all forms of life would flourish.\" And he\\'s right.What TED celebrates is the gift of the human imagination. We have to be careful now that we use this gift wisely and that we avert some of the scenarios that we\\'ve talked about. And the only way we\\'ll do it is by seeing our creative capacities for the richness they are and seeing our children for the hope that they are. And our task is to educate their whole being, so they can face this future. By the way — we may not see this future, but they will. And our job is to help them make something of it.Thank you very much.(Applause)',\n",
       " 'description_standardized': 'sir ken robinson make entertaining profoundly moving case creating education system nurture rather undermines creativity',\n",
       " 'title_standardized': 'school kill creativity',\n",
       " 'transcript_standardized': 'good morning youlaughterits great hasnt ive blown away whole thing fact im leavinglaughterthere three theme running conference relevant want talk one extraordinary evidence human creativity presentation weve people variety range second put u place idea whats going happen term future idea may play outi interest education actually find everybody interest education dont find interesting youre dinner party say work education actually youre often dinner party franklylaughterif work education youre askedlaughterand youre never asked back curiously thats strange say somebody know say say work education see blood run face theyre like oh god know melaughtermy one night weeklaughterbut ask education pin wall one thing go deep people right like religion money thing big interest education think huge vested interest partly education thats meant take u future cant grasp think child starting school year retiring nobody clue despite expertise thats parade past four day world look like five year time yet meant educating unpredictability think extraordinaryand third part weve agreed nonetheless really extraordinary capacity child capacity innovation mean sirena last night marvel wasnt seeing could shes exceptional think shes speak exceptional whole childhood person extraordinary dedication found talent contention kid tremendous talent squander pretty ruthlesslyso want talk education want talk creativity contention creativity important education literacy treat statusapplause thank youapplausethat way thank muchlaughterso minute leftlaughterwell born nolaughteri heard great story recently love telling little girl drawing lesson six back drawing teacher said girl hardly ever paid attention drawing lesson teacher fascinated went said drawing girl said im drawing picture god teacher said nobody know god look like girl said minutelaughterwhen son four england actually four everywhere honestlaughterif strict wherever went four year nativity play remember storylaughterno big big story mel gibson sequel may seen itlaughternativity ii james got part joseph thrilled considered one lead part place crammed full agent tshirts james robinson joseph laughter didnt speak know bit three king come come bearing gift gold frankincense myrrh really happened sitting think went sequence talked little boy afterward said ok said yeah wrong switched three boy came fouryearolds tea towel head put box first boy said bring gold second boy said bring myrrh third boy said frank sent thislaughterwhat thing common kid take chance dont know theyll go right theyre frightened wrong dont mean say wrong thing creative know youre prepared wrong youll never come anything original youre prepared wrong time get adult kid lost capacity become frightened wrong run company like stigmatize mistake running national education system mistake worst thing make result educating people creative capacitiespicasso said said child born artist problem remain artist grow believe passionately dont grow creativity grow rather get educated thisi lived stratfordonavon five year ago fact moved stratford los angeles imagine seamless transition waslaughteractually lived place called snitterfield outside stratford shakespeare father born struck new thought dont think shakespeare father dont think shakespeare child shakespeare seven never thought mean seven point somebody english class wasnt helaughterhow annoying would belaughtermust try harderlaughterbeing sent bed dad know shakespeare go bed put pencil downlaughterand stop speaking like thatlaughterits confusing everybodylaughteranyway moved stratford los angeles want say word transition son didnt want come ive got two kid he daughter didnt want come los angeles loved girlfriend england love life sarah hed known monthlaughtermind theyd fourth anniversary long time youre really upset plane said ill never find another girl like sarah rather pleased frankly laughterbecause main reason leaving countrylaughterbut something strike move america travel around world every education system earth hierarchy subject every one doesnt matter go youd think would otherwise isnt top mathematics language humanity bottom art everywhere earth pretty much every system there hierarchy within art art music normally given higher status school drama dance isnt education system planet teach dance everyday child way teach mathematics think rather important think math important dance child dance time theyre allowed body dont miss meetinglaughtertruthfully happens child grow start educate progressively waist focus head slightly one sideif visit education alien say whats public education think youd conclude look output really succeeds everything get brownie point winner think youd conclude whole purpose public education throughout world produce university professor isnt theyre people come top used one therelaughterand like university professor know shouldnt hold highwater mark human achievement theyre form life another form life theyre rather curious say affection there something curious professor experience typically live head live slightly one side theyre disembodied know kind literal way look upon body form transport headslaughterdont way getting head meetingslaughterif want real evidence outofbody experience get along residential conference senior academic pop discotheque final nightlaughterand see grown men woman writhing uncontrollably beatlaughterwaiting end go home write paper itlaughterour education system predicated idea academic ability there reason around world public system education really th century came meet need industrialism hierarchy rooted two ideasnumber one useful subject work top probably steered benignly away thing school kid thing liked ground would never get job right dont music youre going musician dont art wont artist benign advice profoundly mistaken whole world engulfed revolutionand second academic ability really come dominate view intelligence university designed system image think whole system public education around world protracted process university entrance consequence many highlytalented brilliant creative people think theyre thing good school wasnt valued actually stigmatized think cant afford go wayin next year according unesco people worldwide graduating education since beginning history people combination thing weve talked technology transformation effect work demography huge explosion populationsuddenly degree arent worth anything isnt true student degree job didnt job didnt want one didnt want one frankly laughter kid degree often heading home carry playing video game need previous job required ba need phd process academic inflation indicates whole structure education shifting beneath foot need radically rethink view intelligencewe know three thing intelligence one diverse think world way experience think visually think sound think kinesthetically think abstract term think movement secondly intelligence dynamic look interaction human brain heard yesterday number presentation intelligence wonderfully interactive brain isnt divided compartment fact creativity define process original idea value often come interaction different disciplinary way seeing thingsby way there shaft nerve join two half brain called corpus callosum thicker woman following helen yesterday probably woman better multitasking arent there raft research know personal life wife cooking meal home often thankfullylaughterno shes good thing shes cooking shes dealing people phone shes talking kid shes painting ceiling shes openheart surgery im cooking door shut kid phone hook come get annoyed say terry please im trying fry egg herelaughtergive breaklaughteractually know old philosophical thing tree fall forest nobody hears happen remember old chestnut saw great tshirt recently said man speaks mind forest woman hears still wronglaughterand third thing intelligence distinct im new book moment called epiphany based series interview people discovered talent im fascinated people got really prompted conversation wonderful woman maybe people never heard gillian lynne heard shes choreographer everybody know work cat phantom opera shes wonderful used board royal ballet see anyway gillian lunch one day said get dancer interesting school really hopeless school wrote parent said think gillian learning disorder couldnt concentrate fidgeting think theyd say adhd wouldnt adhd hadnt invented point wasnt available conditionlaughterpeople werent aware could thatlaughteranyway went see specialist oakpaneled room mother led sat chair end sat hand minute man talked mother problem gillian school disturbing people homework always late little kid eight end doctor went sat next gillian said ive listened thing mother told need speak privately wait well back wont long went left herbut went room turned radio sitting desk got said mother stand watch minute left room foot moving music watched minute turned mother said mr lynne gillian isnt sick shes dancer take dance schooli said happened said cant tell wonderful walked room full people like people couldnt sit still people move think move think ballet tap jazz modern contemporary eventually auditioned royal ballet school became soloist wonderful career royal ballet eventually graduated royal ballet school founded gillian lynne dance company met andrew lloyd webber shes responsible successful musical theater production history shes given pleasure million shes multimillionaire somebody else might put medication told calm downapplausewhat think come al gore spoke night ecology revolution triggered rachel carson believe hope future adopt new conception human ecology one start reconstitute conception richness human capacity education system mined mind way stripmine earth particular commodity future wont serve u rethink fundamental principle educating childrenthere wonderful quote jonas salk said insect disappear earth within year life earth would end human being disappeared earth within year form life would flourish he rightwhat ted celebrates gift human imagination careful use gift wisely avert scenario weve talked way well seeing creative capacity richness seeing child hope task educate whole face future way may see future job help make something itthank muchapplause'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 1775\n",
      "Validation set size: 198\n",
      "Test set size: 494\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into a 80/10/10 train/validation/test split\n",
    "train_test_split = dataset[\"train\"].train_test_split(test_size=0.2)\n",
    "\n",
    "# Extract the training and testing sets\n",
    "train_val_dataset = train_test_split[\"train\"]\n",
    "test_dataset = train_test_split[\"test\"]\n",
    "\n",
    "# Further split the training and validation set into 90/10\n",
    "train_val_split = train_val_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# Extract the final training, validation, and test sets\n",
    "train_dataset = train_val_split[\"train\"]\n",
    "validation_dataset = train_val_split[\"test\"]\n",
    "\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(validation_dataset)}\")\n",
    "print(f\"Test set size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": validation_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "train_data = dataset[\"train\"]\n",
    "val_data = dataset[\"validation\"]\n",
    "test_data = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': 'In a lively show, mathemagician Arthur Benjamin races a team of calculators to figure out 3-digit squares, solves another massive mental equation and guesses a few birthdays. How does he do it? He’ll tell you.',\n",
       " 'tags': \"['education', 'entertainment', 'magic', 'math', 'performance']\",\n",
       " 'title': 'A performance of \"Mathemagic\"',\n",
       " 'ratings': \"[{'id': 22, 'name': 'Fascinating', 'count': 3710}, {'id': 9, 'name': 'Ingenious', 'count': 1944}, {'id': 8, 'name': 'Informative', 'count': 340}, {'id': 10, 'name': 'Inspiring', 'count': 817}, {'id': 1, 'name': 'Beautiful', 'count': 429}, {'id': 7, 'name': 'Funny', 'count': 2152}, {'id': 2, 'name': 'Confusing', 'count': 168}, {'id': 21, 'name': 'Unconvincing', 'count': 84}, {'id': 11, 'name': 'Longwinded', 'count': 114}, {'id': 3, 'name': 'Courageous', 'count': 182}, {'id': 23, 'name': 'Jaw-dropping', 'count': 7196}, {'id': 25, 'name': 'OK', 'count': 314}, {'id': 24, 'name': 'Persuasive', 'count': 128}, {'id': 26, 'name': 'Obnoxious', 'count': 151}]\",\n",
       " 'transcript': 'Good morning, ladies and gentlemen. My name is Art Benjamin, and I am a \"mathemagician.\" What that means is, I combine my loves of math and magic to do something I call \"mathemagics.\"But before I get started, I have a quick question for the audience. By any chance, did anyone happen to bring with them this morning a calculator? Seriously, if you have a calculator with you, raise your hand. Raise your hand. Did your hand go up? Now bring it out, bring it out. Anybody else? I see, I see one way in the back. You sir, that\\'s three. And anybody on this side here? OK, over there on the aisle. Would the four of you please bring out your calculators, then join me up on stage. Let\\'s give them a nice round of applause.(Applause)That\\'s right. Now, since I haven\\'t had the chance to work with these calculators, I need to make sure that they are all working properly. Would somebody get us started by giving us a two-digit number, please? How about a two-digit number?Audience: 22.AB: 22. And another two-digit number, sir?Audience: 47.AB: Multiply 22 times 47, make sure you get 1,034, or the calculators are not working. Do all of you get 1,034? 1,034?Volunteer: No.AB: 594. Let\\'s give three of them a nice round of applause there.(Applause)Would you like to try a more standard calculator, just in case? OK, great. What I\\'m going to try and do then — I notice it took some of you a little bit of time to get your answer. That\\'s OK. I\\'ll give you a shortcut for multiplying even faster on the calculator. There is something called the square of a number, which most of you know is taking a number and multiplying it by itself. For instance, five squared would be?Audience: 25.AB: 25. The way we can square on most calculators — let me demonstrate with this one — is by taking the number, such as five, hitting \"times\" and then \"equals,\" and on most calculators that will give you the square. On some of these ancient RPN calculators, you\\'ve got an \"x squared\" button on it, will allow you to do the calculation even faster. What I\\'m going to try and do now is to square, in my head, four two-digit numbers faster than they can do on their calculators, even using the shortcut method. What I\\'ll use is the second row this time, and I\\'ll get four of you to each yell out a two-digit number, and if you would square the first number, and if you would square the second, the third and the fourth, I will try and race you to the answer. OK? So quickly, a two-digit number please.Audience: 37.Arthur Benjamin: 37 squared, OK.Audience: 23.AB: 23 squared, OK.Audience: 59.AB: 59 squared, OK, and finally?Audience: 93.AB: 93 squared. Would you call out your answers, please?Volunteer: 1369. AB: 1369.Volunteer: 529. AB: 529.Volunteer: 3481. AB: 3481.Volunteer: 8649.AB: Thank you very much.(Applause)Let me try to take this one step further. I\\'m going to try to square some three-digit numbers this time. I won\\'t even write these down — I\\'ll just call them out as they\\'re called out to me. Anyone I point to, call out a three-digit number. Anyone on our panel, verify the answer. Just give some indication if it\\'s right. A three-digit number, sir, yes?Audience: 987.AB: 987 squared is 974,169.(Laughter)AB: Yes? Good. Another three-digit —(Applause) — another three-digit number, sir?Audience: 457.AB: 457 squared is 205,849. 205,849? AB: Yes? OK, another, another three-digit number, sir?Audience: 321.AB: 321 is 103,041. 103,041. Yes? One more three-digit number please.Audience: Oh, 722.AB: 722 is 500, that\\'s a harder one. Is that 513,284?Volunteer: Yes.AB: Yes? Oh, one more, one more three-digit number please.Audience: 162. 162 squared is 26,244.Volunteer: Yes.Thank you very much.(Applause)(Applause ends)Let me try to take this one step further.(Laughter)I\\'m going to try to square a four-digit number this time. You can all take your time on this; I will not beat you to the answer on this one, but I will try to get the answer right. To make this a little bit more random, let\\'s take the fourth row this time, let\\'s say, one, two, three, four. If each of you would call out a single digit between zero and nine, that will be the four-digit number that I\\'ll square.Nine.Seven.Five.Eight. 9,758, this will take me a little bit of time, so bear with me. 95 million —(Sighs) 218,564?Volunteer: Yes!AB: Thank you very much.(Applause)(Applause ends)Now, I would attempt to square a five-digit number — and I can — but unfortunately, most calculators cannot.(Laughter)Eight-digit capacity — don\\'t you hate that? So, since we\\'ve reached the limits of our calculators — what\\'s that? Does yours go higher?Volunteer: I don\\'t know.AB: Oh, yours does?Volunteer: I can probably do it. AB: I\\'ll talk to you later. In the meanwhile, let me conclude the first part of my show by doing something a little trickier. Let\\'s take the largest number on the board here, 8649. Would you each enter that on your calculator? And instead of squaring it this time, I want you to take that number and multiply it by any three-digit number that you want, but don\\'t tell me what you\\'re multiplying by — just multiply it by any random three-digit number. So you should have as an answer either a six-digit or probably a seven-digit number. How many digits do you have, six or seven?Seven, and yours?Seven? Seven? And, uncertain.Seven. Is there any possible way that I could know what seven-digit numbers you have? Say \"No.\"(Laughter)Good, then I shall attempt the impossible — or at least the improbable. What I\\'d like each of you to do is to call out for me any six of your seven digits, any six of them, in any order you\\'d like.(Laughter)One digit at a time, I shall try and determine the digit you\\'ve left out. Starting with your seven-digit number, call out any six of them please.Volunteer: 1, 9, 7, 0, 4, 2.AB: Did you leave out the number 6?Good, OK, that\\'s one. You have a seven-digit number, call out any six of them please.Volunteer: 4, 4, 8, 7, 5.I think I only heard five numbers. I — wait — 44875 — did you leave out the number 6?Same as she did, OK. You\\'ve got a seven-digit number — call out any six of them loud and clear.Volunteer: 0, 7, 9, 0, 4, 4.I think you left out the number 3?AB: That\\'s three. The odds of me getting all four of these right by random guessing would be one in 10,000: 10 to the fourth power. OK, any six of them.(Laughter)Really scramble them up this time, please.Volunteer: 2, 6, 3, 9, 7, 2.Did you leave out the number 7? And let\\'s give all four of these people a nice round of applause. Thank you very much.(Applause)(Applause ends) For my next number —(Laughter)while I mentally recharge my batteries, I have one more question for the audience. By any chance, does anybody here happen to know the day of the week that they were born on? If you think you know your birth day, raise your hand. Let\\'s see, starting with — let\\'s start with a gentleman first. What year was it, first of all? That\\'s why I pick a gentleman first.Audience: 1953.1953, and the month? November what? 23rd — was that a Monday?Audience: Yes.Good. Somebody else? I haven\\'t seen any women\\'s hands up. OK, how about you, what year? 1949, and the month? October what? Fifth — was that a Wednesday? Yes! I\\'ll go way to the back right now, how about you? Yell it out, what year?Audience: 1959.1959, OK — and the month?Audience: February. February what?Sixth — was that a Friday? Audience: Yes.Good, how about the person behind her? Call out, what year was it?Audience: 1947. AB: 1947, and the month?Audience: May. AB: May what?Seventh — would that be a Wednesday?Audience: Yes. AB: Thank you very much.(Applause)Anybody here who\\'d like to know the day of the week they were born? We can do it that way. Of course, I could just make up an answer and you wouldn\\'t know, so I come prepared for that. I brought with me a book of calendars. It goes as far back into the past as 1800, because you never know.(Laughter)I didn\\'t mean to look at you, sir — you were just sitting there.(Laughter)Anyway, Chris, you can help me out here, if you wouldn\\'t mind. This is a book of calendars. Who wanted to know their birth day? What year was it, first of all?Audience: 1966.66 — turn to the calendar with 1966. And what month?Audience: April. AB: April what?Audience: 17th.I believe that was a Sunday. Can you confirm, Chris?Chris Anderson: Yes.AB: I\\'ll tell you what, Chris: as long as you have that book in front of you, do me a favor, turn to a year outside of the 1900s, either into the 1800s or way into the 2000s — that\\'ll be a much greater challenge for me. AB: What year would you like? CA: 1824.AB: 1824, OK. AB: And what month?CA: June.AB: June what? CA: Sixth.AB: Was that a Sunday?CA: It was. AB: And it was cloudy.(Laughter)Good, thank you very much.(Applause)(Applause ends)But I\\'d like to wrap things up now by alluding to something from earlier in the presentation. There was a gentleman up here who had a 10-digit calculator. Where is he, would you stand up, 10-digit guy? OK, stand up for me just for a second, so I can see where you are. You have a 10-digit calculator, sir, as well? OK, what I\\'m going to try and do, is to square in my head a five-digit number requiring a 10-digit calculator. But to make my job more interesting for you, as well as for me, I\\'m going to do this problem thinking out loud. So you can actually, honestly hear what\\'s going on in my mind while I do a calculation of this size.Now, I have to apologize to our magician friend Lennart Green. I know as a magician we\\'re not supposed to reveal our secrets, but I\\'m not too afraid that people are going to start doing my show next week, so — I think we\\'re OK.(Laughter)(Applause)So, let\\'s see, let\\'s take a different row of people, starting with you. I\\'ll get five digits: one, two, three, four. Oh, I did this row already. Let\\'s do the row before you, starting with you: one, two, three, four, five. Call out a single digit — that will be the five-digit number that I will try to square, go ahead.Five.Seven.Six.Eight.Three. 57,683 — squared. Yuck.Let me explain to you how I\\'m going to attempt this problem. I\\'m going to break the problem down into three parts. I\\'ll do 57,000 squared, plus 683 squared, plus 57,000 times 683 times two. Add all those numbers together, and with any luck, arrive at the answer. Now, let me recap.(Laughter)Thank you.(Laughter)While I explain something else —(Laughter)— I know, that you can use, right?(Laughter)While I do these calculations, you might hear certain words, as opposed to numbers, creep into the calculation. Let me explain what that is. This is a phonetic code, a mnemonic device that I use, that allows me to convert numbers into words. I store them as words, and later on retrieve them as numbers. I know it sounds complicated; it\\'s not. I don\\'t want you to think you\\'re seeing something out of \"Rain Man.\"(Laughter)There\\'s definitely a method to my madness — definitely, definitely. Sorry.(Laughter)If you want to talk to me about ADHD afterwards, you can talk to me then. By the way, one last instruction, for my judges with the calculators — you know who you are — there is at least a 50 percent chance that I will make a mistake here. If I do, don\\'t tell me what the mistake is; just say, \"you\\'re close,\" or something like that, and I\\'ll try and figure out the answer — which could be pretty entertaining in itself. If, however, I am right, whatever you do, don\\'t keep it to yourself, OK?(Laughter)Make sure everybody knows that I got the answer right, because this is my big finish, OK. So, without any more stalling, here we go. I\\'ll start the problem in the middle, with 57 times 683. 57 times 68 is 3,400, plus 476 is 3876, that\\'s 38,760 plus 171, 38,760 plus 171 is 38,931. 38,931; double that to get 77,862. 77,862 becomes cookie fission, cookie fission is 77,822. That seems right, I\\'ll go on. Cookie fission, OK. Next, I do 57 squared, which is 3,249, so I can say, three billion. Take the 249, add that to cookie, 249, oops, but I see a carry coming — 249 — add that to cookie, 250 plus 77, is 327 million — fission, fission, OK, finally, we do 683 squared, that\\'s 700 times 666, plus 17 squared is 466,489, rev up if I need it, rev up, take the 466, add that to fission, to get, oh gee — 328,489.Audience: Yeah! AB: Good.Thank you very much.(Applause) I hope you enjoyed mathemagics.Thank you.(Applause)',\n",
       " 'description_standardized': 'lively show mathemagician arthur benjamin race team calculator figure digit square solves another massive mental equation guess birthday hell tell',\n",
       " 'title_standardized': 'performance mathemagic',\n",
       " 'transcript_standardized': 'good morning lady gentleman name art benjamin mathemagician mean combine love math magic something call mathemagicsbut get started quick question audience chance anyone happen bring morning calculator seriously calculator raise hand raise hand hand go bring bring anybody else see see one way back sir thats three anybody side ok aisle would four please bring calculator join stage let give nice round applauseapplausethats right since havent chance work calculator need make sure working properly would somebody get u started giving u twodigit number please twodigit numberaudience ab another twodigit number siraudience ab multiply time make sure get calculator working get volunteer noab let give three nice round applause thereapplausewould like try standard calculator case ok great im going try notice took little bit time get answer thats ok ill give shortcut multiplying even faster calculator something called square number know taking number multiplying instance five squared would beaudience ab way square calculator let demonstrate one taking number five hitting time equal calculator give square ancient rpn calculator youve got x squared button allow calculation even faster im going try square head four twodigit number faster calculator even using shortcut method ill use second row time ill get four yell twodigit number would square first number would square second third fourth try race answer ok quickly twodigit number pleaseaudience arthur benjamin squared okaudience ab squared okaudience ab squared ok finallyaudience ab squared would call answer pleasevolunteer ab volunteer ab volunteer ab volunteer ab thank muchapplauselet try take one step im going try square threedigit number time wont even write ill call theyre called anyone point call threedigit number anyone panel verify answer give indication right threedigit number sir yesaudience ab squared laughterab yes good another threedigit applause another threedigit number siraudience ab squared ab yes ok another another threedigit number siraudience ab yes one threedigit number pleaseaudience oh ab thats harder one volunteer yesab yes oh one one threedigit number pleaseaudience squared volunteer yesthank muchapplauseapplause endslet try take one step furtherlaughterim going try square fourdigit number time take time beat answer one try get answer right make little bit random let take fourth row time let say one two three four would call single digit zero nine fourdigit number ill squareninesevenfiveeight take little bit time bear million sigh volunteer yesab thank muchapplauseapplause endsnow would attempt square fivedigit number unfortunately calculator cannotlaughtereightdigit capacity dont hate since weve reached limit calculator whats go highervolunteer dont knowab oh doesvolunteer probably ab ill talk later meanwhile let conclude first part show something little trickier let take largest number board would enter calculator instead squaring time want take number multiply threedigit number want dont tell youre multiplying multiply random threedigit number answer either sixdigit probably sevendigit number many digit six sevenseven yoursseven seven uncertainseven possible way could know sevendigit number say nolaughtergood shall attempt impossible least improbable id like call six seven digit six order youd likelaughterone digit time shall try determine digit youve left starting sevendigit number call six pleasevolunteer ab leave number good ok thats one sevendigit number call six pleasevolunteer think heard five number wait leave number ok youve got sevendigit number call six loud clearvolunteer think left number ab thats three odds getting four right random guessing would one fourth power ok six themlaughterreally scramble time pleasevolunteer leave number let give four people nice round applause thank muchapplauseapplause end next number laughterwhile mentally recharge battery one question audience chance anybody happen know day week born think know birth day raise hand let see starting let start gentleman first year first thats pick gentleman firstaudience month november rd mondayaudience yesgood somebody else havent seen womens hand ok year month october fifth wednesday yes ill go way back right yell yearaudience ok monthaudience february february whatsixth friday audience yesgood person behind call year itaudience ab monthaudience may ab may whatseventh would wednesdayaudience yes ab thank muchapplauseanybody whod like know day week born way course could make answer wouldnt know come prepared brought book calendar go far back past never knowlaughteri didnt mean look sir sitting therelaughteranyway chris help wouldnt mind book calendar wanted know birth day year first allaudience turn calendar monthaudience april ab april whataudience thi believe sunday confirm chrischris anderson yesab ill tell chris long book front favor turn year outside either way thatll much greater challenge ab year would like ca ab ok ab monthca juneab june ca sixthab sundayca ab cloudylaughtergood thank muchapplauseapplause endsbut id like wrap thing alluding something earlier presentation gentleman digit calculator would stand digit guy ok stand second see digit calculator sir well ok im going try square head fivedigit number requiring digit calculator make job interesting well im going problem thinking loud actually honestly hear whats going mind calculation sizenow apologize magician friend lennart green know magician supposed reveal secret im afraid people going start show next week think oklaughterapplauseso let see let take different row people starting ill get five digit one two three four oh row already let row starting one two three four five call single digit fivedigit number try square go aheadfivesevensixeightthree squared yucklet explain im going attempt problem im going break problem three part ill squared plus squared plus time time two add number together luck arrive answer let recaplaughterthank youlaughterwhile explain something else laughter know use rightlaughterwhile calculation might hear certain word opposed number creep calculation let explain phonetic code mnemonic device use allows convert number word store word later retrieve number know sound complicated dont want think youre seeing something rain manlaughtertheres definitely method madness definitely definitely sorrylaughterif want talk adhd afterwards talk way one last instruction judge calculator know least percent chance make mistake dont tell mistake say youre close something like ill try figure answer could pretty entertaining however right whatever dont keep oklaughtermake sure everybody know got answer right big finish ok without stalling go ill start problem middle time time plus thats plus plus double get becomes cookie fission cookie fission seems right ill go cookie fission ok next squared say three billion take add cookie oops see carry coming add cookie plus million fission fission ok finally squared thats time plus squared rev need rev take add fission get oh gee audience yeah ab goodthank muchapplause hope enjoyed mathemagicsthank youapplause'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"validation\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['description',\n",
       " 'tags',\n",
       " 'title',\n",
       " 'ratings',\n",
       " 'transcript',\n",
       " 'description_standardized',\n",
       " 'title_standardized',\n",
       " 'transcript_standardized']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the feature names \n",
    "feature_names = list(dataset[\"train\"].features.keys()) \n",
    "feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Putting it all togother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, load_dataset\n",
    "from pathlib import Path\n",
    "data_directory = \"artifacts\\data_standardization\\standardized_data.csv\"\n",
    "\n",
    "def load_data_into_DatasetDict(data_directory: Path, dataset_type: str = \"csv\") -> DatasetDict:    \n",
    "    dataset = load_dataset(dataset_type, data_files=data_directory)\n",
    "    # Split the dataset into a 80/10/10 train/validation/test split\n",
    "    train_test_split = dataset[\"train\"].train_test_split(test_size=0.2)\n",
    "\n",
    "    # Extract the training and testing sets\n",
    "    train_val_dataset = train_test_split[\"train\"]\n",
    "    test_dataset = train_test_split[\"test\"]\n",
    "\n",
    "    # Further split the training and validation set into 90/10\n",
    "    train_val_split = train_val_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "    # Extract the final training, validation, and test sets\n",
    "    train_dataset = train_val_split[\"train\"]\n",
    "    validation_dataset = train_val_split[\"test\"]\n",
    "    \n",
    "    dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": validation_dataset,\n",
    "    \"test\": test_dataset\n",
    "    })  \n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['description', 'tags', 'title', 'ratings', 'transcript', 'description_standardized', 'title_standardized', 'transcript_standardized'],\n",
       "        num_rows: 1775\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['description', 'tags', 'title', 'ratings', 'transcript', 'description_standardized', 'title_standardized', 'transcript_standardized'],\n",
       "        num_rows: 198\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['description', 'tags', 'title', 'ratings', 'transcript', 'description_standardized', 'title_standardized', 'transcript_standardized'],\n",
       "        num_rows: 494\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_data_into_DatasetDict(data_directory=data_directory)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Single sentence\n",
    "single_sentence = \"This is a single sentence.\"\n",
    "inputs = tokenizer(single_sentence, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "\n",
    "# Batch of sentences\n",
    "batch_sentences = [\n",
    "    \"This is the first sentence.\",\n",
    "    \"This is a slightly longer second sentence.\",\n",
    "    \"Short sentence.\",\n",
    "]\n",
    "batch_inputs = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(batch_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AI\\NLP\\HandsOn\\Text Summarization\\textsummarizer-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer ,AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import pipeline\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "import nltk\n",
    "from datasets import DatasetDict, load_dataset\n",
    "from pathlib import Path\n",
    "from transformers import DataCollatorForSeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Summarizer:\n",
    "    def __init__(self, checkpoint=\"google-t5/t5-small\", max_length=1024, min_length=40):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.checkpoint = checkpoint\n",
    "        self.max_length = max_length\n",
    "        self.min_length = min_length\n",
    "        self.prefix = \"summarize: \"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint).to(self.device)\n",
    "        self.data_collator = DataCollatorForSeq2Seq(tokenizer=self.tokenizer , model= self.model ,padding=True, max_length= self.max_length)\n",
    "        self.training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=\"artifacts\\models\\Summarizer_model_artifacts\",\n",
    "            eval_strategy=\"epoch\",\n",
    "            learning_rate=2e-5,\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=16,\n",
    "            weight_decay=0.01,\n",
    "            save_total_limit=3,\n",
    "            num_train_epochs=4,\n",
    "            predict_with_generate=True,\n",
    "            fp16=True,\n",
    "            push_to_hub=False,\n",
    "        )\n",
    "\n",
    "        \n",
    "    def load_data_into_DatasetDict(self, data_directory: Path, dataset_type: str = \"csv\") -> DatasetDict:    \n",
    "        dataset = load_dataset(dataset_type, data_files=data_directory)\n",
    "        \n",
    "        # Ensure required columns exist\n",
    "        required_features = ['transcript_standardized', 'description_standardized', 'title_standardized']\n",
    "        feature_names = list(dataset[\"train\"].features.keys()) \n",
    "        assert all(col in feature_names for col in required_features), f\"Missing required columns: {required_features}\"     \n",
    "\n",
    "        # Remove unnecessary colummn \n",
    "        dataset = dataset.remove_columns(['description', 'tags', 'title', 'ratings', 'transcript'])           \n",
    "        \n",
    "        # Standerizing colummn names\n",
    "        dataset  = dataset.rename_column(\"transcript_standardized\", \"text\")\n",
    "        dataset  = dataset.rename_column(\"description_standardized\", \"summary\")\n",
    "        dataset  = dataset.rename_column(\"title_standardized\", \"title\")\n",
    "\n",
    "        \n",
    "        \n",
    "        # Split the dataset into a 80/10/10 train/validation/test split\n",
    "        train_test_split = dataset[\"train\"].train_test_split(test_size=0.2)\n",
    "\n",
    "        # Extract the training and testing sets\n",
    "        train_val_dataset = train_test_split[\"train\"]\n",
    "        test_dataset = train_test_split[\"test\"]\n",
    "\n",
    "        # Further split the training and validation set into 90/10\n",
    "        train_val_split = train_val_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "        # Extract the final training, validation, and test sets\n",
    "        train_dataset = train_val_split[\"train\"]\n",
    "        validation_dataset = train_val_split[\"test\"]\n",
    "        \n",
    "        dataset = DatasetDict({\n",
    "        \"train\": train_dataset,\n",
    "        \"validation\": validation_dataset,\n",
    "        \"test\": test_dataset\n",
    "        })  \n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def preprocess_function(self, dataset):\n",
    "        if isinstance(dataset, DatasetDict):\n",
    "            for split in dataset:\n",
    "                dataset[split] = dataset[split].map(self._preprocess_single_split)\n",
    "            return dataset\n",
    "        else:\n",
    "            return self._preprocess_single_split(dataset)\n",
    "\n",
    "    def _preprocess_single_split(self, batch):\n",
    "        if \"text\" not in batch or \"summary\" not in batch:\n",
    "            raise KeyError(f\"Keys 'text' or 'summary' not found. Available keys: {list(batch.keys())}\")\n",
    "        \n",
    "        inputs = [self.prefix + doc for doc in batch[\"text\"]]\n",
    "        model_inputs = self.tokenizer(inputs, padding=True, max_length=1024, truncation=True)\n",
    "        \n",
    "        labels = self.tokenizer(text_target=batch[\"summary\"], padding=True, max_length=128, truncation=True)\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "\n",
    "\n",
    "    def tokenize_dataset(self,dataset):\n",
    "        tokenized_dataset = dataset.map(self.preprocess_function, batched=True)\n",
    "        return tokenized_dataset\n",
    "    \n",
    "    \n",
    "\n",
    "    def compute_metrics(self, eval_pred):\n",
    "        rouge = evaluate.load(\"rouge\")\n",
    "        predictions, labels = eval_pred\n",
    "        decoded_preds = self.tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        labels = np.where(labels != -100, labels, self.tokenizer.pad_token_id)\n",
    "        decoded_labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "        prediction_lens = [np.count_nonzero(pred != self.tokenizer.pad_token_id) for pred in predictions]\n",
    "        result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "        return {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "    def model_trainer(self,dataset):\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=self.model,\n",
    "            args=self.training_args,\n",
    "            train_dataset=dataset[\"train\"],\n",
    "            eval_dataset=dataset[\"test\"],\n",
    "            tokenizer=self.tokenizer,\n",
    "            data_collator=self.data_collator,\n",
    "            compute_metrics=self.compute_metrics,\n",
    "        )\n",
    "        trainer.train()\n",
    "        return trainer\n",
    "\n",
    "    def predict(self, text: str,) -> str:\n",
    "        \"\"\"Generate summary for a given text using the trained model\"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            self.prefix + text,\n",
    "            max_length=self.max_length or self.max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.device)\n",
    "        \n",
    "        summary_ids = self.model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_length=self.max_length,\n",
    "            min_length=self.min_length,\n",
    "            num_beams=4,\n",
    "            length_penalty=2.0,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        \n",
    "        return self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\AI\\\\NLP\\\\HandsOn\\\\Text Summarization'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(r\"D:\\AI\\NLP\\HandsOn\\Text Summarization\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = Summarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1775/1775 [03:58<00:00,  7.43 examples/s]\n",
      "Map: 100%|██████████| 198/198 [00:27<00:00,  7.27 examples/s]\n",
      "Map: 100%|██████████| 494/494 [01:07<00:00,  7.27 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = summarizer.load_data_into_DatasetDict(data_directory = \"artifacts\\data_standardization\\standardized_data.csv\")\n",
    "model_inputs = summarizer.preprocess_function(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<bound method Summarizer.preprocess_function of <__main__.Summarizer object at 0x000002230538A310>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Map: 100%|██████████| 20/20 [00:00<00:00, 282.99 examples/s]\n",
      "Map: 100%|██████████| 20/20 [00:00<00:00, 338.78 examples/s]\n",
      "Map: 100%|██████████| 20/20 [00:00<00:00, 300.08 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = summarizer.tokenize_dataset(model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_trainer_sampeler(dataset: Dataset, sample_size: int = 20):\n",
    "    sampled_dataset_random = dataset\n",
    "    for split in dataset:\n",
    "        sampled_dataset_random[split] = dataset[split].shuffle(seed=42).select(range(sample_size)) \n",
    "\n",
    "    return sampled_dataset_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['summary', 'title', 'text', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 20\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['summary', 'title', 'text', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 20\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['summary', 'title', 'text', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 20\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = data_trainer_sampeler(tokenized_dataset)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enay9\\AppData\\Local\\Temp\\ipykernel_22868\\2951465384.py:104: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "  0%|          | 0/8 [03:25<?, ?it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]d:\\AI\\NLP\\HandsOn\\Text Summarization\\textsummarizer-env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2691: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      " 25%|██▌       | 2/8 [00:29<01:19, 13.18s/it]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "                                             \n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "\u001b[A                                          \n",
      "\n",
      "\n",
      " 25%|██▌       | 2/8 [01:30<01:19, 13.18s/it]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 10.442031860351562, 'eval_rouge1': 0.0734, 'eval_rouge2': 0.0174, 'eval_rougeL': 0.0617, 'eval_rougeLsum': 0.0609, 'eval_gen_len': 20.0, 'eval_runtime': 61.2928, 'eval_samples_per_second': 0.326, 'eval_steps_per_second': 0.033, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4/8 [02:03<02:04, 31.03s/it]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "                                             \n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "\u001b[A                                          \n",
      "\n",
      "\n",
      " 50%|█████     | 4/8 [02:10<02:04, 31.03s/it]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 10.170769691467285, 'eval_rouge1': 0.0689, 'eval_rouge2': 0.0174, 'eval_rougeL': 0.0604, 'eval_rougeLsum': 0.0596, 'eval_gen_len': 20.0, 'eval_runtime': 6.9208, 'eval_samples_per_second': 2.89, 'eval_steps_per_second': 0.289, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 6/8 [02:31<00:40, 20.32s/it]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "                                             \n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "\u001b[A                                          \n",
      "\n",
      "\n",
      " 75%|███████▌  | 6/8 [03:08<00:40, 20.32s/it]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 9.887848854064941, 'eval_rouge1': 0.0689, 'eval_rouge2': 0.0174, 'eval_rougeL': 0.0604, 'eval_rougeLsum': 0.0596, 'eval_gen_len': 20.0, 'eval_runtime': 36.5351, 'eval_samples_per_second': 0.547, 'eval_steps_per_second': 0.055, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [03:32<00:00, 23.37s/it]d:\\AI\\NLP\\HandsOn\\Text Summarization\\textsummarizer-env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2691: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "                                             \n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "\u001b[A                                          \n",
      "\n",
      "\n",
      "100%|██████████| 8/8 [04:23<00:00, 23.37s/it]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "                                             \n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "100%|██████████| 8/8 [04:23<00:00, 23.37s/it]\n",
      "100%|██████████| 8/8 [04:23<00:00, 32.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 9.818323135375977, 'eval_rouge1': 0.0649, 'eval_rouge2': 0.0174, 'eval_rougeL': 0.0563, 'eval_rougeLsum': 0.0554, 'eval_gen_len': 20.0, 'eval_runtime': 37.2787, 'eval_samples_per_second': 0.536, 'eval_steps_per_second': 0.054, 'epoch': 4.0}\n",
      "{'train_runtime': 263.5021, 'train_samples_per_second': 0.304, 'train_steps_per_second': 0.03, 'train_loss': 10.653341293334961, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Summarizer' object has no attribute 'get_summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m summarizer\u001b[38;5;241m.\u001b[39mmodel_trainer(sample)\n\u001b[0;32m      2\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHere we are developing a state-of-the-art summarization pipeline that is designed to tirelessly summarize TED Talks covering a diverse range of topics and concepts.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[43msummarizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_summary\u001b[49m(test)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(summary)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Summarizer' object has no attribute 'get_summary'"
     ]
    }
   ],
   "source": [
    "trainer = summarizer.model_trainer(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI summarization uses natural language processing to condense lengthy texts into concise summaries. this technology finds diverse applications, from streamlining news consumption to optimizing business workflows.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "test = \"Imagine a world overwhelmed by information, where sifting through endless articles, reports, and data consumes valuable time and energy. AI summarization offers a powerful solution, employing natural language processing to condense lengthy texts into concise summaries. These systems utilize two primary approaches: extractive summarization, selecting key sentences directly from the source, and abstractive summarization, generating new, more human-like summaries that capture the core meaning. This technology finds diverse applications, from streamlining news consumption and accelerating research analysis to optimizing business workflows by summarizing meetings and customer feedback. While challenges remain in handling complex language, nuanced meanings, and ensuring factual accuracy, ongoing research continually improves the fluency, coherence, and contextual understanding of AI-generated summaries. Ultimately, AI summarization promises to revolutionize how we process information, empowering us to access key insights quickly and efficiently, unlocking the potential of knowledge for a more informed future.\"\n",
    "summary = summarizer.predict(test)\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textsummarizer-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
